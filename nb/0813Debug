# è¼‰å…¥æ‚¨çš„å°èªæ•¸æ“š
from datasets import load_dataset
data_file = "/content/drive/MyDrive/train_chatml.jsonl"
dataset = load_dataset("json", data_files=data_file, split="train")

print(f"âœ… è¼‰å…¥ {len(dataset)} ç­†å°èªæ•¸æ“š")
print(f"ğŸ“Š æ•¸æ“šæ¬„ä½: {dataset.column_names}")
print(f"ğŸ“‹ ç¯„ä¾‹: {dataset[0]}")


# æª¢æŸ¥æ•¸æ“šæ ¼å¼ä¸¦è™•ç†
if "text" in dataset.column_names:
    # æ‚¨çš„æ•¸æ“šæ˜¯ ChatML æ ¼å¼ï¼Œéœ€è¦è½‰æ›
    def convert_chatml_to_harmony(examples):
        texts = []
        for text in examples["text"]:
            import re
            
            # è§£æ ChatML
            system_match = re.search(r'<\|start\|>system\n(.*?)<\|end\|>', text, re.DOTALL)
            user_match = re.search(r'<\|start\|>user\n(.*?)<\|end\|>', text, re.DOTALL)
            assistant_match = re.search(r'<\|start\|>assistant\n(.*?)<\|end\|>', text, re.DOTALL)
            
            if system_match and user_match and assistant_match:
                system_content = system_match.group(1).strip()
                user_content = user_match.group(1).strip()
                assistant_content = assistant_match.group(1).strip()
                
                # æ·»åŠ æ¨ç†è¨­å®š
                enhanced_system = f"{system_content}\n\nReasoning: medium"
                
                conversation = [
                    {"role": "system", "content": enhanced_system},
                    {"role": "user", "content": user_content},
                    {"role": "assistant", "content": assistant_content}
                ]
                
                # è½‰æ›ç‚º harmony format
                harmony_text = tokenizer.apply_chat_template(
                    conversation, tokenize=False, add_generation_prompt=False
                )
                texts.append(harmony_text)
            else:
                texts.append(text)
        return {"text": texts}
    
    dataset = dataset.map(convert_chatml_to_harmony, batched=True)
    print("âœ… ChatML è½‰æ›ç‚º harmony format å®Œæˆ")

elif "messages" in dataset.column_names:
    # å¦‚æœæ˜¯æ¨™æº–æ ¼å¼ï¼Œä½¿ç”¨åŸå§‹è™•ç†
    from unsloth.chat_templates import standardize_sharegpt
    dataset = standardize_sharegpt(dataset)
    dataset = dataset.map(formatting_prompts_func, batched=True)

output_dir = "gpt-oss-20b-taiwanese-assistant",


messages = [
    {"role": "system", "content": "ä½ æ˜¯å°èªç¿»è­¯åŠ©æ‰‹\n\nReasoning: medium"},
    {"role": "user", "content": "è«‹ç¿»è­¯ï¼šä»Šå¤©å¤©æ°£å¾ˆå¥½"},
]


# æª¢æŸ¥ tokenizer èˆ‡ model vocab å°ºå¯¸
print(f"Model vocab size: {model.config.vocab_size}")
print(f"Tokenizer vocab size: {tokenizer.vocab_size}")

# å–å‡º input_ids
ids = inputs["input_ids"]

# æª¢æŸ¥æœ€å°å€¼èˆ‡æœ€å¤§å€¼
min_id = ids.min().item()
max_id = ids.max().item()
print(f"Min token id: {min_id}")
print(f"Max token id: {max_id}")

# åˆ¤æ–·æ˜¯å¦è¶Šç•Œ
if max_id >= model.config.vocab_size or min_id < 0:
    print("âš ï¸ Token ID è¶…å‡ºæ¨¡å‹è©è¡¨ç¯„åœï¼Œè«‹æª¢æŸ¥ tokenizer èˆ‡ model æ˜¯å¦åŒ¹é…ï¼")
else:
    print("âœ… Token ID åœ¨åˆæ³•ç¯„åœå…§ã€‚")


inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
    return_dict=True
).to(model.device)
